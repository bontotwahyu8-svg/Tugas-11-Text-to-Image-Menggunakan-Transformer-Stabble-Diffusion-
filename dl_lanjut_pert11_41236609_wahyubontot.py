# -*- coding: utf-8 -*-
"""DL lanjut pert11 41236609_WahyuBontot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18uENyVHzGDr90im9EXYK4bItPtWKfvlu
"""

!pip install datasets diffusers transformers accelerate
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_dataset

# 1. Load Dataset
print("Memuatdataset...")
raw_dataset= load_dataset("reach-vb/pokemon-blip-captions", split="train")

# 2. Ambil daftar caption untukproses adaptasi teks
all_captions= [item['text'] for item in raw_dataset]

# 3. Setup Text Vectorization
max_tokens = 5000
seq_len = 20
text_vectorizer = layers.TextVectorization(
max_tokens=max_tokens,
output_sequence_length=seq_len,
)
# Proses Adapt (Mempelajari kosakata dari dataset)
text_vectorizer.adapt(all_captions)
vocab = text_vectorizer.get_vocabulary()
print(f"Kamus Teks Berhasil Dibuat. Jumlah kosakata: {len(vocab)}")
print("Contoh 10 kata pertama:", vocab[:10])

def preprocess_fn(item):
    # Proses Gambar
    image = item['image'].convert("RGB").resize((64, 64))
    image = np.array(image) / 255.0  # Normalisasi0-1
    # Proses Teks
    caption = item['text']
    return caption, image
# Membuatgenerator dataset
def gen():
    for item in raw_dataset:
        yield preprocess_fn(item)
# Membuattf.data.Dataset
train_ds= tf.data.Dataset.from_generator(
    gen,
    output_signature=(
        tf.TensorSpec(shape=(), dtype=tf.string),
        tf.TensorSpec(shape=(64, 64, 3), dtype=tf.float32)
    )
)
# Batching dan TransformasiTeks keAngka
train_ds= train_ds.map(lambda x, y: (text_vectorizer(x), y))
train_ds= train_ds.batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)

class PokemonTrainer(keras.Model):
    def __init__(self, transformer, vqvae_encoder):
        super().__init__()
        self.transformer= transformer
        self.vqvae_encoder= vqvae_encoder
        self.loss_tracker= keras.metrics.Mean(name="loss")

    def train_step(self, data):
        text_tokens, images = data
        # 1. Ubahgambaraslimenjaditoken visual menggunakanencoder
        # Kita simulasikandenganoutput dummy sesuaiukuranlatent grid (misal16x16)
        visual_tokens= tf.random.uniform((tf.shape(images)[0], 256), minval=0, maxval=1024, dtype=tf.int32)
        # 2. Siapkaninput dan target (Autoregressive)
        vis_input= visual_tokens[:, :-1]
        vis_target= visual_tokens[:, 1:]
        with tf.GradientTape() as tape:
            # Prediksi
            preds = self.transformer([text_tokens, vis_input], training=True)
            # HitungLoss
            loss = keras.losses.sparse_categorical_crossentropy(vis_target, preds, from_logits=True)
        grads = tape.gradient(loss, self.transformer.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.transformer.trainable_variables))
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

# Placeholder for VQVAE Encoder
# In a real scenario, this would be a trained VQ-VAE encoder
# that takes an image and outputs visual tokens.
# For now, it's just a dummy Keras Model that can be passed.
class DummyVQVAEEncoder(keras.Model):
    def __init__(self):
        super().__init__()
        # It doesn't need to do anything complex for this error fix
        # as its output is mocked in train_step
        self.dummy_layer = layers.Lambda(lambda x: x)

    def call(self, inputs):
        # We simulate the output of the VQ-VAE encoder in train_step
        # so this model doesn't need to actually perform the encoding here.
        # It just needs to be a valid Keras Model instance.
        return self.dummy_layer(inputs)

vqvae_encoder = DummyVQVAEEncoder()

# Placeholder for Transformer Model
# This model needs to accept text tokens and visual input tokens
# and predict the next visual token.
# Based on train_step:
# text_tokens shape: (batch_size, seq_len)
# vis_input shape: (batch_size, 255) (visual_tokens[:, :-1])
# preds shape: (batch_size, 255, 1024) (vocab size for visual tokens is 1024)
text_input = keras.Input(shape=(seq_len,), dtype="int32", name="text_input")
visual_input = keras.Input(shape=(255,), dtype="int32", name="visual_input")

# Simple embedding for text tokens
text_embedding_dim = 256
visual_embedding_dim = 256
visual_token_vocab_size = 1024 # based on maxval for visual_tokens

text_embedded = layers.Embedding(input_dim=max_tokens, output_dim=text_embedding_dim)(text_input)
visual_embedded = layers.Embedding(input_dim=visual_token_vocab_size, output_dim=visual_embedding_dim)(visual_input)

# Pool each sequence independently to a fixed-size vector
text_pooled = layers.GlobalAveragePooling1D()(text_embedded) # Shape: (None, 256)
visual_pooled = layers.GlobalAveragePooling1D()(visual_embedded) # Shape: (None, 256)

# Now concatenate these pooled features
combined_features = layers.Concatenate(axis=-1)([text_pooled, visual_pooled]) # Shape: (None, 512)

# Apply dense layer to the combined features
x = layers.Dense(512, activation="relu")(combined_features)

# The transformer output needs to predict for each visual token in the sequence.
# This part remains the same, transforming the combined context into a sequence of predictions.
x = layers.Dense(255 * visual_token_vocab_size, activation="relu")(x)
output_logits = layers.Reshape((255, visual_token_vocab_size))(x)

transformer_model = keras.Model(inputs=[text_input, visual_input], outputs=output_logits, name="transformer_model")

# Inisialisasidan Compile
trainer = PokemonTrainer(transformer_model, vqvae_encoder)
trainer.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))

def generate_pokemon(prompt):
    # 1. Ubahtekskeangka
    tokenized_text= text_vectorizer([prompt])
    # 2. Generate token visual (Autoregressive)
    gen_vis_tokens= generate_image_tokens(transformer_model, tokenized_text, 256, 1024)
    return gen_vis_tokens # Assuming this function should return the generated visual tokens

# Function definition for generate_image_tokens
def generate_image_tokens(model, text_tokens, num_visual_tokens_output_len, visual_token_vocab_size):
    # num_visual_tokens_output_len is 256 (the desired total length of visual tokens)
    # The transformer model's visual_input expects 255 tokens.
    # The transformer model's output provides 255 predictions.

    batch_size = tf.shape(text_tokens)[0] # Should be 1 for single prompt generation

    # Initial visual input to the transformer.
    # Since we need to produce num_visual_tokens_output_len (256) tokens in total,
    # and the transformer predicts 255 tokens based on 255 input tokens,
    # we'll generate the 255 tokens and prepend a start token.
    # We provide a 'blank' or 'start-of-sequence' visual input to kick off generation.
    # Let's use 0 for padding/start token for now.
    visual_input_for_transformer = tf.zeros((batch_size, num_visual_tokens_output_len - 1), dtype=tf.int32) # (1, 255)

    # Get predictions from the transformer
    # The model outputs logits for 255 tokens: (batch_size, 255, visual_token_vocab_size)
    predictions_logits = model([text_tokens, visual_input_for_transformer], training=False)

    # Get the token IDs by taking argmax for each position
    generated_tokens_from_model = tf.argmax(predictions_logits, axis=-1, output_type=tf.int32) # (batch_size, 255)

    # Prepend a start token to make the sequence 256 long.
    # Assuming 0 can serve as a start token, or an unused token ID.
    start_token = tf.zeros((batch_size, 1), dtype=tf.int32)
    full_generated_visual_tokens = tf.concat([start_token, generated_tokens_from_model], axis=-1) # (batch_size, 256)

    return full_generated_visual_tokens

# Placeholder for decode_to_real_image
def decode_to_real_image(visual_tokens):
    # In a real scenario, this would use a pre-trained VQ-VAE decoder
    # to reconstruct an image from the visual tokens.
    # For now, let's return a dummy image.
    # The output image should have shape (height, width, channels), e.g., (64, 64, 3)
    batch_size = tf.shape(visual_tokens)[0]
    # Creating a dummy image (e.g., a pink square) for demonstration.
    # Assuming the desired output is a single image for now.
    dummy_image = tf.random.uniform((1, 64, 64, 3), minval=0.0, maxval=1.0, dtype=tf.float32)
    # If the tokens represent an image, we might want to visualize them as a grid or a single image.
    # For simplicity, returning a fixed dummy image for now.
    return tf.squeeze(dummy_image, axis=0) # Remove batch dimension if only one image

# 3. Decode jadiGambar menggunakanPre-trained VAE
# Gunakanfungsidecode_to_real_imageyang memanggilAutoencoderKL
gen_vis_tokens = generate_pokemon("a pink cute pokemon") # Call generate_pokemon to get gen_vis_tokens
final_image= decode_to_real_image(gen_vis_tokens)
plt.imshow(final_image)
plt.title("a pink cute pokemon") # Use the prompt for the title
plt.axis("off")
plt.show()
# 5. JalankanTraining
print("MemulaiPelatihan...")
trainer.fit(train_ds, epochs=100)